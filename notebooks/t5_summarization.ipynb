{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T00:44:04.974575Z",
     "iopub.status.busy": "2023-03-07T00:44:04.974253Z",
     "iopub.status.idle": "2023-03-07T00:44:28.565436Z",
     "shell.execute_reply": "2023-03-07T00:44:28.564078Z",
     "shell.execute_reply.started": "2023-03-07T00:44:04.974547Z"
    }
   },
   "outputs": [],
   "source": [
    "# colab installs\n",
    "\n",
    "#!pip install transformers==4.26.1\n",
    "#!pip install lightning==1.9.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lightning as pl\n",
    "import lightning\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers.models.t5.tokenization_t5_fast import T5Tokenizer\n",
    "from transformers import \\\n",
    "    T5ForConditionalGeneration, \\\n",
    "    T5TokenizerFast as T5Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T00:44:52.638897Z",
     "iopub.status.busy": "2023-03-07T00:44:52.638497Z",
     "iopub.status.idle": "2023-03-07T00:44:53.678212Z",
     "shell.execute_reply": "2023-03-07T00:44:53.677062Z",
     "shell.execute_reply.started": "2023-03-07T00:44:52.638848Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T00:45:10.142343Z",
     "iopub.status.busy": "2023-03-07T00:45:10.141891Z",
     "iopub.status.idle": "2023-03-07T00:45:26.530697Z",
     "shell.execute_reply": "2023-03-07T00:45:26.529434Z",
     "shell.execute_reply.started": "2023-03-07T00:45:10.142297Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join(\"data\", \"cnn_dailymail\", \"train.csv\") \n",
    "test_filepath = os.path.join(\"cnn_dailymail\", \"test.csv\") \n",
    "val_filepath = os.path.join(\"cnn_dailymail\", \"validation.csv\") \n",
    "df = pd.read_csv(filepath)#[:4000]\n",
    "df_val = pd.read_csv(val_filepath)#[:400]\n",
    "df_test = pd.read_csv(test_filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T00:45:26.533045Z",
     "iopub.status.busy": "2023-03-07T00:45:26.532302Z",
     "iopub.status.idle": "2023-03-07T00:45:26.584276Z",
     "shell.execute_reply": "2023-03-07T00:45:26.583295Z",
     "shell.execute_reply.started": "2023-03-07T00:45:26.532999Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = df[[\"article\", \"highlights\"]]\n",
    "df_train.columns = [\"text\", \"summary\"]\n",
    "df_test = df_test[[\"article\", \"highlights\"]]\n",
    "df_test.columns = [\"text\", \"summary\"]\n",
    "df_val = df_val[[\"article\", \"highlights\"]]\n",
    "df_val.columns = [\"text\", \"summary\"]\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T00:45:26.586353Z",
     "iopub.status.busy": "2023-03-07T00:45:26.585913Z",
     "iopub.status.idle": "2023-03-07T00:45:26.596527Z",
     "shell.execute_reply": "2023-03-07T00:45:26.595423Z",
     "shell.execute_reply.started": "2023-03-07T00:45:26.586310Z"
    }
   },
   "outputs": [],
   "source": [
    "class SummaryDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,\\\n",
    "                data, tokenizer,\\\n",
    "                text_max_length=512,\\\n",
    "                summary_max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.text_max_length = text_max_length\n",
    "        self.summary_max_length = summary_max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        data_row = self.data.iloc[index]\n",
    "        \n",
    "        text = data_row[\"text\"]\n",
    "        summary = data_row[\"summary\"]\n",
    "        \n",
    "        text_encoding = self.tokenizer(\\\n",
    "                text,\\\n",
    "                max_length=self.text_max_length,\\\n",
    "                padding=\"max_length\",\\\n",
    "                return_attention_mask=True,\\\n",
    "                add_special_tokens=True,\\\n",
    "                return_tensors=\"pt\",\\\n",
    "                truncation=True)\n",
    "        \n",
    "        summary_encoding = self.tokenizer(\\\n",
    "                summary,\\\n",
    "                max_length=self.summary_max_length,\\\n",
    "                padding=\"max_length\",\\\n",
    "                return_attention_mask=True,\\\n",
    "                add_special_tokens=True,\\\n",
    "                return_tensors=\"pt\",\\\n",
    "                truncation=True)\n",
    "        \n",
    "        labels = summary_encoding[\"input_ids\"]\n",
    "        labels[labels == 0] = -100\n",
    "        \n",
    "        return {\"text\": text,\\\n",
    "                \"input_ids\": text_encoding[\"input_ids\"],\\\n",
    "                \"summary\": summary,\\\n",
    "                \"text_attention_mask\": text_encoding[\"attention_mask\"],\\\n",
    "                \"labels\": labels.flatten(),\\\n",
    "                \"labels_attention_mask\": summary_encoding[\"attention_mask\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T00:45:26.599210Z",
     "iopub.status.busy": "2023-03-07T00:45:26.598192Z",
     "iopub.status.idle": "2023-03-07T00:45:26.610645Z",
     "shell.execute_reply": "2023-03-07T00:45:26.609667Z",
     "shell.execute_reply.started": "2023-03-07T00:45:26.599167Z"
    }
   },
   "outputs": [],
   "source": [
    "class SummaryPLDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(\\\n",
    "            self,\\\n",
    "            df_train, \\\n",
    "            df_val, \\\n",
    "            tokenizer, \\\n",
    "            batch_size=2, \\\n",
    "            text_max_length=512, \\\n",
    "            summary_max_length=128 \\\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.df_train = df_train\n",
    "        self.df_val = df_val\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.text_max_length = text_max_length\n",
    "        self.summary_max_length = summary_max_length\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        self.train_dataset = SummaryDataset(\\\n",
    "                self.df_train,\\\n",
    "                self.tokenizer,\\\n",
    "                self.text_max_length,\\\n",
    "                self.summary_max_length \\\n",
    "                )\n",
    "        self.val_dataset = SummaryDataset(\\\n",
    "                self.df_val,\\\n",
    "                self.tokenizer,\\\n",
    "                self.text_max_length,\\\n",
    "                self.summary_max_length \\\n",
    "                )\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        \n",
    "        return DataLoader(\\\n",
    "                self.train_dataset, \\\n",
    "                batch_size=self.batch_size, \\\n",
    "                shuffle=True, \\\n",
    "                num_workers=16 \\\n",
    "                )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        \n",
    "        return DataLoader(\\\n",
    "                self.val_dataset, \\\n",
    "                batch_size=self.batch_size, \\\n",
    "                shuffle=False, \\\n",
    "                num_workers=16 \\\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T00:45:27.047164Z",
     "iopub.status.busy": "2023-03-07T00:45:27.046452Z",
     "iopub.status.idle": "2023-03-07T00:45:27.062383Z",
     "shell.execute_reply": "2023-03-07T00:45:27.061246Z",
     "shell.execute_reply.started": "2023-03-07T00:45:27.047124Z"
    }
   },
   "outputs": [],
   "source": [
    "class Summarizer(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, lr=1e-4, model_name=\"t5-base\"):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.model= T5ForConditionalGeneration.from_pretrained(\\\n",
    "                model_name,\\\n",
    "                return_dict=True \\\n",
    "                )\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.summary_max_length = 128\n",
    "        self.text_max_length = 512\n",
    "        self.number_beams = 1\n",
    "        self.use_early_stopping = True\n",
    "        \n",
    "    def forward(self, input_ids, \\\n",
    "            attention_mask, decoder_attention_mask,\\\n",
    "            labels=None):\n",
    "        \n",
    "        my_output = self.model(input_ids, \\\n",
    "                attention_mask=attention_mask, \\\n",
    "                labels=labels, \\\n",
    "                decoder_attention_mask=decoder_attention_mask \\\n",
    "                )\n",
    "        \n",
    "        return my_output.loss, my_output.logits\n",
    "    \n",
    "    def training_step(self, batch, batch_index=None):\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"][:,0,...]\n",
    "        attention_mask = batch[\"text_attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        labels_attention_mask = batch[\"labels_attention_mask\"][:,0,...]\n",
    "        \n",
    "        loss, outputs = self.forward(input_ids=input_ids,\\\n",
    "                attention_mask=attention_mask,\\\n",
    "                decoder_attention_mask=labels_attention_mask,\\\n",
    "                labels=labels\\\n",
    "                )\n",
    "        \n",
    "        self.log(\"training_loss\", loss)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_index=None):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            input_ids = batch[\"input_ids\"][:,0,...]\n",
    "            attention_mask = batch[\"text_attention_mask\"]\n",
    "            labels = batch[\"labels\"]\n",
    "            labels_attention_mask = batch[\"labels_attention_mask\"][:,0,...]\n",
    "\n",
    "            loss, outputs = self.forward(input_ids=input_ids,\\\n",
    "                    attention_mask=attention_mask,\\\n",
    "                    decoder_attention_mask=labels_attention_mask,\\\n",
    "                    labels=labels\\\n",
    "                    )\n",
    "\n",
    "\n",
    "            self.log(\"validation_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        return torch.optim.Adam(\\\n",
    "                self.parameters(),\\\n",
    "                lr=self.lr)\n",
    "    \n",
    "    def summarize(self, text, tokenizer):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            text_encoding = tokenizer(\\\n",
    "                    text,\\\n",
    "                    max_length=self.text_max_length,\\\n",
    "                    padding=\"max_length\",\\\n",
    "                    return_attention_mask=True,\\\n",
    "                    add_special_tokens=True,\\\n",
    "                    return_tensors=\"pt\",\\\n",
    "                    truncation=True\\\n",
    "                                            )\n",
    "            input_ids = text_encoding[\"input_ids\"]\n",
    "            attention_mask = text_encoding[\"attention_mask\"]\n",
    "\n",
    "\n",
    "            summary_ids = self.model.generate(\\\n",
    "                    input_ids=input_ids,\\\n",
    "                    attention_mask=attention_mask,\\\n",
    "                    max_length=self.summary_max_length,\\\n",
    "                    num_beams=self.number_beams,\\\n",
    "                    repetition_penalty=3.0,\\\n",
    "                    length_penalty=1.0,\\\n",
    "                    early_stopping=self.use_early_stopping\\\n",
    "                               )\n",
    "\n",
    "            decoded_list = [tokenizer.decode(id_elem,\\\n",
    "                    clean_up_tokenization_spaces=True,\\\n",
    "                    skip_special_tokens=True) for id_elem in summary_ids]\n",
    "\n",
    "            decoded = \"\".join(decoded_list)\n",
    "\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T00:45:26.617446Z",
     "iopub.status.busy": "2023-03-07T00:45:26.616503Z",
     "iopub.status.idle": "2023-03-07T00:45:27.037052Z",
     "shell.execute_reply": "2023-03-07T00:45:27.034827Z",
     "shell.execute_reply.started": "2023-03-07T00:45:26.617415Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model_name= \"t5-base\"\n",
    "model_name= \"t5-small\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "\n",
    "model = Summarizer(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T00:45:27.039165Z",
     "iopub.status.busy": "2023-03-07T00:45:27.038536Z",
     "iopub.status.idle": "2023-03-07T00:45:27.044712Z",
     "shell.execute_reply": "2023-03-07T00:45:27.043272Z",
     "shell.execute_reply.started": "2023-03-07T00:45:27.039130Z"
    }
   },
   "outputs": [],
   "source": [
    "number_epochs = 1\n",
    "batch_size = 4\n",
    "\n",
    "data_module = SummaryPLDataModule(df_train, \\\n",
    "        df_val,\\\n",
    "        tokenizer, \\\n",
    "        batch_size=batch_size \\\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=number_epochs,\\\n",
    "        accelerator=\"gpu\",\\\n",
    "        devices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "number_samples = 10\n",
    "\n",
    "for sample_nubmer in range(number_samples):\n",
    "    sample_index = np.random.randint(len(df_val))\n",
    "    sample_text = df_val.iloc[sample_index][\"text\"]\n",
    "\n",
    "    sample_summary = df_val.iloc[sample_index][\"summary\"]\n",
    "\n",
    "    \n",
    "    summarized = model.summarize(sample_text, tokenizer)\n",
    "    #print(f\"\\nFull text: \\n    {sample_text}\")\n",
    "    print(f\"\\nSummary target: \\n    {sample_summary}\")\n",
    "\n",
    "    print(f\"\\nSummarized text: \\n    {summarized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
